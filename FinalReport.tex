\documentclass{article}
\usepackage{setspace}   %Allows double spacing with the \doublespacing command
\usepackage{lipsum} % Add dummy text
\usepackage{graphicx}
\begin{document}

\title{Population prediction based on satellite imagery\\
Term Project Report} 

\date{December 2017}
\author{Yassine Kadiri, Santiago Novoa, Zsolt Pajor-Gyulai, Manuel Serrano}
\maketitle

\doublespacing


\section{Introduction}
In this project, we attempted to build models predicting population density based on satellite imagery inspired by \cite{RHD17}. We achieve this by comparing spatially disaggregated census data on the continental United States (that is excluding Alaska and U.S. territories) to satellite images of the particular region. We use several approaches: 
\begin{itemize}
\item[(1)] naive logistic regression on the vectorized satellite images; 
\item[(2)] convolutional neural network(CNN) built from scratch; \item[(3)] pre-trained CNN developed for image recognition (Vgg16).
\end{itemize}

While our models do not achieve particularly high accuracy, they show considerable lift corresponding to random guessing. 

\section{Business understanding}
The 'business' in question in this case is mostly non-profit, governmental application. In order to allocate resources, government agencies require knowledge on the geographical distribution of the country's population. In the absence of direct measurements in years between two censuses, these entities are forced to rely on ad-hoc methods to model the evolution of the population since the last census year (\cite{L96}).

Our project explores the possibility of obtaining direct measurements of the population of a particular area by looking at Satellite imagery. Such capability would be of great value in many decision making processes such as urban development, evacuation planning, and gauging future demand for food, water, energy, and services. For example, according to the US General Accounting Office, more than 70 federal programs distribute tens of billions of dollars annually on the basis of population estimates.

Furthermore, censuses in many countries are non-representative due to limited civil registration systems or are outright fraudulent (\cite{BDYPHN06}). In this case, having an independent way to estimate population could be beneficial in the optimal allocation of humanitarian aid or for clandestine purposes.

\section{Data}
\subsection{Data understanding}

\subsection{Data preparation}
\section{Modeling and evaluation}
Our goal in this section is to show that we can extract predictive value from our data, that is, our models provide a non-unit lift compared to random guessing.
\subsection{Logistic regression}
\subsection{Convolutional Neural Network from scratch}
This section describes the development of a Deep Convolutional Neural Network from scratch, and fully coded in Python using the State-of-the-art Deep Learning framework, Tensorflow.

In this case, this networks has been coded to perform a \textbf{regression} problem, not a classification. For measuring the accuracy, the same population intervals used in the VGG approach has been used in order to define if a given prediction lays in its correct interval or not.

 \subsubsection{Architecture}
 
 The high-level architecture of the network is described below:
 \begin{enumerate}
\item \textbf{Convolutional layer 1:} three channels as input, 8 feature maps as output. ReLU activation function. Kernel size of 3x3. Stride of 1x1.
\item \textbf{Maxpooling layer:} Pool size and stride of 2x2.
\item \textbf{Convolutional layer 2:} 16 feature maps as output. ReLU activation function. Kernel size of 3x3. Stride of 1x1.
\item \textbf{Maxpooling layer:} Pool size and stride of 2x2.
\item \textbf{Convolutional layer 3:} 16 feature maps as output. ReLU activation function. Kernel size of 3x3. Stride of 1x1.
\item \textbf{Maxpooling layer:} Pool size and stride of 2x2.
\item \textbf{Convolutional layer 4:} 32 feature maps as output. ReLU activation function. Kernel size of 3x3. Stride of 1x1.
\item \textbf{Maxpooling layer:} Pool size and stride of 2x2.
\item \textbf{Reshape:} Flatten output from fourth convolutional layer.
\item \textbf{Fully connected layer 1:} 512 neurons. ReLU activation function.
\item \textbf{Fully connected layer 2:} 128 neurons. ReLU activation function.
\item \textbf{Output:} population estimation. One output.
 \end{enumerate}

The images fed into the network have been 200x200x3 (RGB) pixels, and in batches of 64 images randomly selected from the training set, and without replacement.
 
\subsubsection{Optimizers}
In order to perform the best possible prediction, different optimizers have been tested. These are:
\begin{itemize}
\item \textbf{Adadelta optimizer:}\footnote{http://ruder.io/optimizing-gradient-descent/index.html\#adadelta} It is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size [1].
\item \textbf{Adam optimizer:}\footnote{http://ruder.io/optimizing-gradient-descent/index.html\#adam}
 Stands for Adaptive Moment Estimation, and it is a method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients, similar to momentum [2].
\end{itemize}


Different learning rates have been tested on both optimizers, and the final rates used have been: $\eta = 0.1$ for Adadelta, and $\eta = 0.001$ for Adam.

\subsubsection{Training details}
Different training techniques have been applied in order to compare the performances and choose the best. These are:
\begin{itemize}
\item \textbf{Oversampling:} Since the amount of images in every class defined for accuracy is not equal, an oversampling technique has been perform in order to uniformly select images from all classes when creating the random training batch at every training iteration.
\item \textbf{Similar distribution in train and test sets:} In order to have a less biased training set, the split between training and test images has been perform in such a way that both interval distributions are similar.
\end{itemize}
 
With respect to the software and hardware equipment, Tensorflow 1.3 and CUDA 8 have been running using NVIDIA GTX 1080 GPU.
 	
 \subsubsection{Results}
 
 \begin{figure}[ht] 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{loss_adam.png} 
    \caption{Loss with Adam} 
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{acc_adam.png} 
    \caption{Accuracy with Adam}
  \end{minipage} 
\end{figure}

\begin{figure}[ht] 
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{loss_adadel.png} 
    \caption{Loss with Adadelta} 
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{acc_adadel.png} 
    \caption{Accuracy with Adadelta}
  \end{minipage} 
\end{figure}

 \begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{loss_adam_oversample.png} 
    \caption{Loss Adam oversampling} 
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{acc_adam_oversample.png} 
    \caption{Acc. Adam oversampling}
  \end{minipage} 
\end{figure}

\begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{loss_adadel_oversample.png} 
    \caption{Loss Adadelta oversampling} 
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[width=1.1\linewidth]{acc_adadel_oversample.png} 
    \caption{Acc. Adadelta oversampling}
  \end{minipage} 
\end{figure}

\subsubsection{Conclusions}
As we can see from the results above, the best accuracies are achieved using both Adam and Adadelta optimizers (surprisingly) without oversampling, but splitting test and test sets keeping the similar interval distributions. 








\subsection{Pretrained VGG architecture}
The Vgg16 architecture is a 16 layer CNN developed for large scale image recognition by the Visual Geometry Group at the University of Oxford (\cite{SZ14}). This architecture won the ILSVRC-2014 ImageNet competition in 2014. We use the Keras version that has been obtained by directly converting the Caffe model provided by the authors\footnote{https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3, however, we use the modified version adapted to the Fast.ai course.}.

We used this CNN to perform multi-type classification with respect to eight population intervals:
\begin{figure}[ht]
\center
\begin{tabular}{ |c|c| } 
\hline
Population interval & Number of images\\
\hline
0-1& 154\\ 
1-10& 782\\ 
10-50& 2813\\
50-100& 908\\
100- 500& 1294\\
500-1000& 529\\
1000-2000& 401\\
2000-11000& 119\\
Total& 7000\\
\hline
\end{tabular}
\caption{Population intervals and the number of corresponding images in our dataset of images. The upper end of the intervals is inclusive, while the lower end is excluded, except for the very first interval.}
\end{figure}

Vgg16 is designed to assign probability scores of an image belonging to 1000 different ImageNet categories. We finetune the network according to our population intervals, that is we use the ImageNet scores as features to predict our population intervals (so essentially we assign scores that a satellite image is a dog, cat, corn, ketchup, etc. and then use these scores as inputs for classification). After performing a random $80\%-20\%$ split between the training and validation set, we performed 30 training epochs using the ADAM\footnote{https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/} algorithm with learning rate $0.001$ and categorical cross-entropy as our loss function. This gave us validation accuracy of $62.29\%$ and validation loss $1.0054$. Moreover the confusion matrix on the validation set is:
\begin{figure}[ht]
\centering
\includegraphics[scale=0.45]{conf_mx_Zsolt1.png}
\end{figure}

To see if we can improve performance, we also attempted to retrain the ImageNet layer of the model (using the finetuned model above, to avoid starting with random weights in the final layer, which would cause the retrained layer to quickly move a long way from its optimized ImageNet weights): This gave us  a lower validation accuracy of $60.62\%$ and the validation set confusion matrix:

\begin{figure}[ht]
\centering
\includegraphics[scale=0.45]{conf_mx_Zsolt2.png}
\end{figure}

On several preliminary rounds, we have observed the validation accuracy fluctuating between $60-62\%$, which leads us to the conclusion that further training epochs would not improve our model.



\subsection{Comparison of our models}
To compare the accuracy of our models on an independent test set, we considered a balanced set of 168 images close to the area where our original sample was obtained. Since the set is balanced over the population intervals, the accuracy of random guessing is $1/8=12.5\%$. To test our models on a different terrain, we obtain a small balanced test set of 160 images from the San Francisco bay area. The accuracy results are summarized in Figure \ref{fig:accur-comp}.

\begin{figure}[ht]
\center
\begin{tabular}{ |c|c|c| } 
\hline
Model & EC & WC\\
\hline
LogReg& &\\ 
Manu& &\\ 
VggFineTune& $42.26\%$ & $32.5\%$\\
VggRetrain& $45.83\%$ & $30.62\%$\\
\hline
\end{tabular}
\caption{Accuracies of our different models on the same test sets.}
\label{fig:accur-comp}
\end{figure}

As expected, the models perform significantly worse on the geographical area different from that in the training set. These accuracy figures nevertheless represent significant improvement over what one would expect from random guessing. We consider this as a successful proof of concept that it is indeed possible to extract information on the population of a particular region from satellite images taken from that area.

\section{Deployment}
While we did not obtain high enough accuracy for reliable use, in this section we describe issues related to deployment after sufficient improvement has been achieved. After a successful validation of our models, deployment should be in the form of an automated software implementation, where decision makers can obtain the figure on the population of a particular area by simply feeding the neural network the corresponding satellite images.

However, every governmental agency using our model for decision making should be aware of the imperfection of our predictions. Accordingly, whenever human life depends on the results (e.g. evacuation planning, or disaster relief funds), the policymakers should make sure to have appropriate cushion built into their actions. To provide further security, the cautious user should use our models in conjunction with other techniques and be alerted by huge discrepancies. This of course applies to all other applications with, perhaps, less severe consequences.

As features of areas populated by humans, in particular overall architecture, change rather slowly over time, one would expect a well trained model to be robust over a long period of time. Of course, the model should go through revalidation or, perhaps, retraining from time to time. A natural point for this to occur are the census years, when labeled data becomes available.

Unless it is the explicit goal of using the model (e.g. in clendestine applications), one has to be aware that obtaining satellite images of people's properties raises obvious privacy issues. The respectful user should use a low enough resolution such that the details of individual's properties cannot be extracted from the image.

\section{Possible further directions}

There are many interesting questions arising in connection with using our models that we did not have the chance to investigate due to the lack of time and data acquisition difficulties.

For example, we could have looked at different (preferrably larger) training sets containing different terrains, e.g., training in California and testing on the east coast. We could have also tried to perform population estimates on foreign countries to see how well our models generalize. For example, it would have been interesting to predict the population of North Korea and see how the measurement would correlate with the official figure. Finally, we could have used different population intervals to see if it increases accuracy.

\appendix
\section{Contribution of each team member to the project}
\subsection{Yassine Kadiri}
\subsection{Santiago Novoa}
\subsection{Zsolt Pajor-Gyulai}
Researched the paper \cite{RHD17}, when the project proposal came up. Worked on the write-up of the project proposal. Learned how to use the Vgg architecture (and how to do any sort of deep learning in the first place) from the online Fast.ai course. Built the Vgg model and wrote the corresponding section in this write-up. Wrote the first version of the 'Summary', 'Business understanding', and 'Deployment' sections of this write-up.
\subsection{Manuel Serrano}

\begin{thebibliography}{9}

\bibitem[BDYPHN06]{BDYPHN06} \textsc{D. Balk, U. Deichmann, G. Yetman, F. Pozzi, S. Hay, A. Nelson} \textit{ Determining global population distribution: methods, applications, and data.} Advances in parasitology, Vol 62, p119-156 (2006)

\bibitem[L96]{L96} \textsc{J. Long}\textit{ Postcensal population estimates: States counties and places.} Indirect Estimators in US Federal Programs, p59-82, Springer (1996)

\bibitem[RHD17]{RHD17}  \textsc{C. Robinson, F. Hohman, B. Dilkina}\textit{ A Deep Learning Approach for Population Estimation from Satellite Imagery.} arXiv preprint arXiv:1708.09086 (2017)

\bibitem[SZ14]{SZ14} \textsc{K. Simonyan, A. Zisserman} \textit{ Very Deep Convolutional Networks for Large-Scale Image Recognition} arXiv preprint arXiv:1409.1556
\end{thebibliography}
\end{document}